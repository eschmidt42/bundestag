{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing votes\n",
    "> Downloading & parsing votes Aafter downloading xlsx files behind the links on `https://www.bundestag.de/parlament/plenum/abstimmung/liste`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "import re, os\n",
    "import typing\n",
    "import requests\n",
    "import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting URIs for `xlsx` documents from `htm` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_data_dir = Path('../raw_data')\n",
    "xlsx_data_dir = Path('../xlsx_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_file_paths(data_dir:typing.Union[Path,str], suffix:str=None, pattern=None): \n",
    "    return list(set([data_dir/f for f in os.listdir(data_dir) if (suffix and f.endswith(suffix)) or (pattern and pattern.search(f))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_file_paths = get_file_paths(html_data_dir, suffix='.htm')\n",
    "html_file_paths[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_file_paths = get_file_paths(html_data_dir, pattern=re.compile('(\\.htm)'))\n",
    "html_file_paths[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def collect_xlsx_uris(html_file_paths:typing.List[Path]):\n",
    "    xlsx_uris = {}\n",
    "    pattern = re.compile('(XLSX?)')\n",
    "\n",
    "    for file_path in html_file_paths:\n",
    "        \n",
    "        with open(file_path, 'r') as f:\n",
    "            soup = BeautifulSoup(f)\n",
    "        \n",
    "        elements = soup.find_all('td', attrs={'data-th':'Dokument'})\n",
    "        for element in elements:\n",
    "            title = element.div.p.strong.text.strip()\n",
    "            href = element.find('a', attrs={'title':pattern})\n",
    "            if href is None: continue\n",
    "            xlsx_uris[title] = href['href']\n",
    "    return xlsx_uris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xlsx_uris = collect_xlsx_uris(html_file_paths)\n",
    "list(xlsx_uris.items())[:3], list(xlsx_uris.items())[-3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download `xlsx` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def download_xlsx_file(xlsx_uri:str, xlsx_dir:typing.Union[Path,str]=Path('../xlsx_data'),\n",
    "                  verbose:bool=False):\n",
    "    fname = xlsx_uri.split('/')[-1]\n",
    "    xlsx_dir = Path(xlsx_dir)\n",
    "    with open(xlsx_dir/fname, 'wb') as f:\n",
    "        r = requests.get(xlsx_uri)\n",
    "        if verbose: print(f'Writing to {xlsx_dir/fname}')\n",
    "        f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsx_uris.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsx_uris.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xlsx_uri = xlsx_uris['10.09.2020: Abstrakte Normenkontrolle - DÃ¼ngeverordnung (Beschlussempfehlung)']\n",
    "download_xlsx_file(xlsx_uri, xlsx_dir=xlsx_data_dir, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def download_multiple_xlsx_files(xlsx_uris:typing.Dict[str,str], xlsx_dir:typing.Union[Path,str]=Path('../xlsx_data'),\n",
    "                            t_sleep:float=.01):\n",
    "    xlsx_file_title_maps = {}\n",
    "    for xlsx_title, xlsx_uri in tqdm.tqdm(xlsx_uris.items(), desc='xlsx File', total=len(xlsx_uris)):\n",
    "        download_xlsx_file(xlsx_uri, xlsx_dir=xlsx_dir)\n",
    "        xlsx_file_title_maps[xlsx_uri.split('/')[-1]] = xlsx_title\n",
    "        time.sleep(t_sleep)\n",
    "    return xlsx_file_title_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xlsx_file_title_maps = download_multiple_xlsx_files(xlsx_uris, xlsx_dir=xlsx_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsx_file_title_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load xlsx into DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsx_files = get_file_paths(xlsx_data_dir, pattern=re.compile('(\\.xlsx?)'))\n",
    "xlsx_files[:3], xlsx_files[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def is_date(s:str, fun:typing.Callable):\n",
    "    try:\n",
    "        _ = fun(s)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def get_xlsx_df(xlsx_file:typing.Union[str,Path], xlsx_file_title_maps:typing.Dict[str,str]=None): \n",
    "    dfs = pd.read_excel(xlsx_file, sheet_name=None)\n",
    "    assert len(dfs) == 1, 'The xlsx file has more than one page, that\\' unexpected.'\n",
    "    for name, df in dfs.items():\n",
    "        df['sheet_name'] = name\n",
    "        #df['date'] = pd.to_datetime(xlsx_file.name.split('_')[0])\n",
    "    if xlsx_file_title_maps is not None:\n",
    "        tmp = xlsx_file_title_maps[xlsx_file.name].split(':')\n",
    "        date = tmp[0]\n",
    "        if is_date(date, lambda x: pd.to_datetime(x, dayfirst=True)):\n",
    "            date = pd.to_datetime(date, dayfirst=True)\n",
    "            title = ':'.join(tmp[1:])\n",
    "        elif is_date(xlsx_file.name.split('_')[0], pd.to_datetime):\n",
    "            date = pd.to_datetime(xlsx_file.name.split('_')[0])\n",
    "            title = xlsx_file_title_maps[xlsx_file.name]\n",
    "        else:\n",
    "            date = None\n",
    "            title = xlsx_file_title_maps[xlsx_file.name]\n",
    "            \n",
    "        df['date'] = date\n",
    "        df['title'] = title\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xlsx_file = xlsx_files[0]\n",
    "get_xlsx_df(xlsx_file, xlsx_file_title_maps=xlsx_file_title_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert isinstance(get_xlsx_df(xlsx_file), pd.DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def collect_xlsxs_as_dict(xlsx_files:typing.List[typing.Union[Path,str]],\n",
    "                          xlsx_file_title_maps:typing.Dict[str,str]=None): \n",
    "    df = []\n",
    "    for xlsx_file in xlsx_files:\n",
    "        df.append(get_xlsx_df(xlsx_file, xlsx_file_title_maps=xlsx_file_title_maps))\n",
    "    return pd.concat(df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = collect_xlsxs_as_dict(xlsx_files, xlsx_file_title_maps=xlsx_file_title_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#df.to_parquet('../votes.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38_web]",
   "language": "python",
   "name": "conda-env-py38_web-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
