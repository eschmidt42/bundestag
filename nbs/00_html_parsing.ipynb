{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing votes\n",
    "> Downloading & parsing votes Aafter downloading xlsx files behind the links on `https://www.bundestag.de/parlament/plenum/abstimmung/liste`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "import re, os\n",
    "import typing\n",
    "import requests\n",
    "import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from loguru import logger\n",
    "from fastcore.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting URIs for `.xlsx`/`.xls` documents from `.htm` files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.xlsx` / `.xls` will be referred as \"sheet\" files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_path = Path('../website_data')\n",
    "sheet_path = Path('../sheets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "RE_HTM = re.compile('(\\.html?)')\n",
    "\n",
    "def get_file_paths(path:typing.Union[Path,str], suffix:str=None, pattern=None): \n",
    "    'Collecting files with a specific suffix or pattern from `path`'\n",
    "    return Path(path).ls().filter(lambda x: x.suffix=='.htm' or (pattern and pattern.search(str(x)))).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_file_paths = get_file_paths(html_path, pattern=RE_HTM)\n",
    "html_file_paths[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert len(html_file_paths) > len(get_file_paths(html_path, suffix='.htm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "RE_SHEET = re.compile('(XLSX?)')\n",
    "\n",
    "def collect_sheet_uris(html_file_paths:typing.List[Path]):\n",
    "    uris = {}\n",
    "    pattern = re.compile('(XLSX?)')\n",
    "\n",
    "    for file_path in tqdm.tqdm(html_file_paths, total=len(html_file_paths), desc='HTM(L)'):\n",
    "        \n",
    "        with open(file_path, 'r') as f:\n",
    "            soup = BeautifulSoup(f, features=\"html.parser\")\n",
    "        \n",
    "        elements = soup.find_all('td', attrs={'data-th':'Dokument'})\n",
    "        for element in elements:\n",
    "            title = element.div.p.strong.text.strip()\n",
    "            href = element.find('a', attrs={'title':RE_SHEET})\n",
    "            if href is None: continue\n",
    "            uris[title] = href['href']\n",
    "    return uris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sheet_uris = collect_sheet_uris(html_file_paths)\n",
    "list(sheet_uris.items())[:3], list(sheet_uris.items())[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert isinstance(sheet_uris, dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading sheet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def download_sheet(uri:str, sheet_path:typing.Union[Path,str],\n",
    "                   verbose:bool=False):\n",
    "    sheet_path.mkdir(exist_ok=True)\n",
    "    path = Path(sheet_path) / uri.split('/')[-1]\n",
    "    with open(path, 'wb') as f:\n",
    "        r = requests.get(uri)\n",
    "        if verbose: print(f'Writing to {path}')\n",
    "        f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert '10.09.2020: Abstrakte Normenkontrolle - Düngeverordnung (Beschlussempfehlung)' in sheet_uris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "uri = sheet_uris['10.09.2020: Abstrakte Normenkontrolle - Düngeverordnung (Beschlussempfehlung)']\n",
    "download_sheet(uri, sheet_path=sheet_path, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def download_multiple_sheets(uris:typing.Dict[str,str], sheet_path:typing.Union[Path,str],\n",
    "                             t_sleep:float=.01, nmax:int=None):\n",
    "    file_title_maps = {uri.split('/')[-1]: title for title, uri in uris.items()}\n",
    "    for i, (title, uri) in tqdm.tqdm(enumerate(uris.items()), desc='File', total=len(uris)):\n",
    "        if nmax is not None and i > nmax: break\n",
    "        if (Path(sheet_path) / uri.split('/')[-1]).exists(): continue\n",
    "        download_sheet(uri, sheet_path=sheet_path)\n",
    "        time.sleep(t_sleep)\n",
    "    return file_title_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "file_title_maps = download_multiple_sheets(sheet_uris, sheet_path=sheet_path, nmax=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert len(file_title_maps) == len(sheet_uris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading sheets into DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting the `xlsx` and `xls` file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "RE_FNAME = re.compile('(\\.xlsx?)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_files = get_file_paths(sheet_path, pattern=RE_FNAME)\n",
    "sheet_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert len(sheet_files) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading files into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def is_date(s:str, fun:typing.Callable):\n",
    "    try:\n",
    "        _ = fun(s)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "VOTE_COLS = ['ja', 'nein', 'Enthaltung', 'ungültig', 'nichtabgegeben']\n",
    "    \n",
    "def get_sheet_df(sheet_file:typing.Union[str,Path], file_title_maps:typing.Dict[str,str]=None): \n",
    "    'Parsing xlsx and xls files into dataframes'\n",
    "    \n",
    "    if Path(sheet_file).stat().st_size == 0:\n",
    "        loguru.warning(f'{sheet_file} is of size 0, skipping ...')\n",
    "        return\n",
    "    \n",
    "    dfs = pd.read_excel(sheet_file, sheet_name=None)\n",
    "    \n",
    "    assert len(dfs) == 1, 'The sheet file has more than one page, that\\'s unexpected.'\n",
    "    \n",
    "    for name, df in dfs.items():\n",
    "        df['sheet_name'] = name\n",
    "    \n",
    "    assert not (df[VOTE_COLS].sum(axis=1) == 0).any()\n",
    "    assert not (df[VOTE_COLS].sum(axis=1) > 1).any()\n",
    "    \n",
    "    date, title = None, None\n",
    "    if file_title_maps is not None:    \n",
    "        title, date = handle_title_and_date(file_title_maps[sheet_file.name], sheet_file)\n",
    "        \n",
    "    df['date'] = date\n",
    "    df['title'] = title\n",
    "        \n",
    "    return df.pipe(disambiguate_party)\n",
    "\n",
    "def handle_title_and_date(full_title:str, sheet_file):\n",
    "    'Extracting the title of the roll call vote and the date'\n",
    "    title = full_title.split(':')\n",
    "    date = title[0]\n",
    "    if is_date(date, lambda x: pd.to_datetime(x, dayfirst=True)):\n",
    "        date = pd.to_datetime(date, dayfirst=True)\n",
    "        title = ':'.join(title[1:])\n",
    "    elif is_date(sheet_file.name.split('_')[0], pd.to_datetime):\n",
    "        date = pd.to_datetime(sheet_file.name.split('_')[0])\n",
    "        title = full_title\n",
    "    else:\n",
    "        date = None\n",
    "        title = full_title\n",
    "    return title, date\n",
    "\n",
    "PARTY_MAP = {'BÜNDNIS`90/DIE GRÜNEN': 'BÜ90/GR', 'DIE LINKE': 'DIE LINKE.', 'fraktionslos': 'Fraktionslos', 'fraktionslose': 'Fraktionslos'}\n",
    "\n",
    "def disambiguate_party(df:pd.DataFrame, col:str='Fraktion/Gruppe', party_map:dict=None):\n",
    "    if party_map is None: party_map = PARTY_MAP\n",
    "    df[col] = df[col].apply(lambda x: x if x not in party_map else party_map[x])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sheet_file = sheet_files[0]\n",
    "df = get_sheet_df(sheet_file, file_title_maps=file_title_maps)\n",
    "df.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert isinstance(df, pd.DataFrame)\n",
    "assert all([col in df.columns.values for col in  ['Wahlperiode', 'Sitzungnr', 'Abstimmnr', 'Fraktion/Gruppe', 'Name',\n",
    "                                                  'Vorname', 'Titel', 'ja', 'nein', 'Enthaltung', 'ungültig', 'nichtabgegeben', \n",
    "                                                  'Bezeichnung', 'sheet_name', 'date', 'title']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Squishing vote columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_squished_dataframe(df:pd.DataFrame, id_col:str='Bezeichnung',\n",
    "                           feature_cols:typing.List[str]=VOTE_COLS,\n",
    "                           other_cols:typing.List=None):\n",
    "    \n",
    "    other_cols = ['date', 'title'] if other_cols is None else other_cols\n",
    "    tmp = df.loc[:, [id_col] + feature_cols + other_cols]\n",
    "    tmp['issue'] = df['date'].dt.date.apply(str) + ' ' + df['title']\n",
    "\n",
    "    tmp = tmp.set_index([id_col, 'issue'] + other_cols)\n",
    "    tmp = (tmp[tmp == 1].stack()\n",
    "            .reset_index()\n",
    "            .drop(0,1)\n",
    "            .rename(columns={f'level_{2+len(other_cols)}':'vote'}))\n",
    "    return df.join(tmp.set_index(['Bezeichnung','date','title']), on=['Bezeichnung','date','title']).drop(columns=VOTE_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_squished = get_squished_dataframe(df)\n",
    "df_squished.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert len(df_squished) == len(df)\n",
    "assert 'vote' in df_squished.columns\n",
    "assert 'issue' in df_squished.columns\n",
    "assert not any([v in df_squished.columns for v in VOTE_COLS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting some dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "DTYPES = {'Wahlperiode':int, 'Sitzungnr': int, 'Abstimmnr': int, 'Fraktion/Gruppe':str, \n",
    "          'Name': str, 'Vorname': str, 'Titel': str, 'vote': str, 'issue': str,\n",
    "#           'ja': bool, 'nein': bool, 'Enthaltung': bool, 'ungültig': bool, 'nichtabgegeben': bool, 'Bemerkung': str,\n",
    "          'Bezeichnung': str, 'sheet_name': str,\n",
    "          'date': 'datetime64[ns]', 'title':str}\n",
    "\n",
    "def set_sheet_dtypes(df:pd.DataFrame):\n",
    "    for col, dtype in DTYPES.items():\n",
    "        df[col] = df[col].astype(dtype)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_squished = set_sheet_dtypes(df_squished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading multiple sheets into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_multiple_sheets_df(sheet_files:typing.List[typing.Union[Path,str]],\n",
    "                           file_title_maps:typing.Dict[str,str]=None): \n",
    "    df = []\n",
    "    for sheet_file in tqdm.tqdm(sheet_files, total=len(sheet_files), desc='Sheets'):\n",
    "        df.append(\n",
    "            (get_sheet_df(sheet_file, file_title_maps=file_title_maps)\n",
    "             .pipe(get_squished_dataframe)\n",
    "             .pipe(set_sheet_dtypes))\n",
    "        )\n",
    "    return pd.concat(df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = get_multiple_sheets_df(sheet_files, file_title_maps=file_title_maps)\n",
    "df.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing all the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_multiple_sheets(html_path, sheet_path, nmax:int=None):\n",
    "    html_path, sheet_path = Path(html_path), Path(sheet_path)\n",
    "    html_file_paths = get_file_paths(html_path, pattern=RE_HTM)\n",
    "    sheet_uris = collect_sheet_uris(html_file_paths)\n",
    "    file_title_maps = download_multiple_sheets(sheet_uris, sheet_path=sheet_path, nmax=nmax)\n",
    "    sheet_files = get_file_paths(sheet_path, pattern=RE_FNAME)\n",
    "    df = get_multiple_sheets_df(sheet_files, file_title_maps=file_title_maps)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = get_multiple_sheets(html_path, sheet_path, nmax=3)\n",
    "df.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# df.to_parquet('../roll_call_votes.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38_fastai]",
   "language": "python",
   "name": "conda-env-py38_fastai-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
